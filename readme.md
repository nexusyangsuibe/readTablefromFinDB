# readTableFromFinDB

作用：配置菜单式读取合并从csmar或cnrds下载的数据包，配置菜单的基本格式见app.py，使用的不同情境见test/app_test.py。

优点：用户从csmar下载数据后无需解压缩，从cnrds下载数据并解压缩后无需亲自从文件夹中遍历，就可以简单快速地使用本程序完成读取拼接数据文件的任务

将csmar格式处理为自动从压缩文件中读取而将cnrds格式处理为需要用户先手动解压缩的考量：
1. csmar下载占用硬盘空间较大的数据文件时通常会采用要求逐年下载多个压缩包的方式，因此需要解压多次，因此使用程序自动解压读取可以节约时间，而cnrds通常采用一个压缩包内部多层次的文件夹目录结构形式。只需解压一次，使用程序解压缩并不节省太多时间；
2. 丰富程序可以处理的数据文件形式，即可以从压缩包中读取也可以直接从文件夹中读取，在这里csmar和cnrds只是作为这两种形式的典型使用场景而这样命名，并不是说只能处理从csmar或cnrds下载的数据，但从非csmar下载压缩包格式数据处理时应注意zip_starts_with不能为"all"或"auto"，需要手动指定，否则程序会自动按照csmar的命名方式查找要处理的压缩文件导致错误）。

特点：采用先确定要读取的数据文件的位置再使用多进程读取最大限度利用现代计算机的多核性能（相较于简单的使用for-loop遍历目录并顺序读写），使用pickle格式存储中间过程以减少过程内存占用来确保大的数据文件可以被顺利高效读取。

其他：有任何的bug或改进意见都欢迎在issue中提出（当然也欢迎通过nexus_yang@126.com邮件私发给我），会持续改进；requirements.txt仅供参考，应该numpy和pandas任何版本都行吧。

已知的bug：
1. 在处理一些较大的数据集时，在concatDF函数结束后，主程序会经过一个较长的等待才执行下一行，或在程序完成最后一行的运行后依然需要等待一段很长的时期才会sys.exit（这两种现象有时会同时出现），且在此期间内存占用量会维持在一个较高的水平，以一个缓慢的速率下降，同时CPU的一个逻辑处理器会保持100%的利用率（我在使用intel i9-13980hx测试时通常是cpu10）。这通常是子进程没有正确关闭的表现，但是我在程序中已经使用了with表达式以自动结束子进程，因此这可能并不是真正的问题所在。当我使用断点调试以探究原因时，问题无法复现，在此过程中也没有发现任何暴露的子进程，在这种情况下我很难找到确切的问题所在。鉴于这个问题仅出现在处理较大的数据集时，我猜想这可能是由于部分子进程在swap分区而非内存中运行导致的一些问题，但我也没有更多证据来表明这一点。由于这个bug的发生较少（我的计算机是64GB内存，可能在更小内存的计算机上会更多发生），我目前没有修复该bug的计划。若用户在使用本程序时遇到了这个问题并想要自行修复，可以尝试手动重写多进程的部分，即将简单的pool.map()改为在每个新的子进程创建前先确定是否有足够的系统资源以完成任务。
